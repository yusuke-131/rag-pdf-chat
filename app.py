import streamlit as st
import fitz  
from langchain.text_splitter import RecursiveCharacterTextSplitter  
from langchain_community.vectorstores import Chroma  
from langchain_community.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI  
from langchain_ollama import OllamaLLM  
from langchain.chains import RetrievalQA  
from langchain.llms.base import LLM  
from huggingface_hub import InferenceClient  
from langchain import PromptTemplate  
import os
from dotenv import load_dotenv  
from datetime import datetime
import traceback
import uuid

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY") # OpenAI APIã‚­ãƒ¼
hf_token = os.getenv("HUGGINGFACEHUB_API_TOKEN") # HuggingFace Hub APIãƒˆãƒ¼ã‚¯ãƒ³


def get_hf_client():
    if not hf_token:
        raise ValueError("`.env` ã« HUGGINGFACEHUB_API_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    return InferenceClient(token=hf_token)


class HFInferenceLLM(LLM):
    @property
    def client(self):
        return get_hf_client()

    def _call(self, prompt, stop=None):
        response = self.client.text_generation(
            model="mistralai/Mistral-7B-Instruct-v0.1",
            inputs=prompt,
            parameters={"temperature": 0.7, "max_new_tokens": 256},
        )
        return response[0]["generated_text"]

    @property
    def _identifying_params(self):
        return {"model": "mistralai/Mistral-7B-Instruct-v0.1"}

    @property
    def _llm_type(self):
        return "huggingface_inference"


def main():
    st.title("ğŸ“„ PDFãƒ™ãƒ¼ã‚¹ã®RAGãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¢")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šLLMã®é¸æŠ
    llm_option = st.sidebar.radio(
        "ä½¿ç”¨ã™ã‚‹LLMã‚’é¸æŠã—ã¦ãã ã•ã„",
        ("OpenAI Chatãƒ¢ãƒ‡ãƒ«", "Ollamaï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼‰"),
        index=0
    )

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šEmbeddingã®ç¨®é¡ã‚’é¸æŠ
    embedding_option = st.sidebar.selectbox("Embeddingsã®ç¨®é¡ã‚’é¸æŠã—ã¦ãã ã•ã„", ["HuggingFaceï¼ˆç„¡æ–™ï¼‰", "OpenAIï¼ˆèª²é‡‘ï¼‰"])

    # APIã‚­ãƒ¼ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
    if ("OpenAI" in llm_option or "OpenAI" in embedding_option) and not api_key:
        st.error("`.env` ã« OPENAI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    if llm_option == "HuggingFace Inferenceï¼ˆç„¡æ–™ï¼‰" and not hf_token:
        st.error("`.env` ã« HUGGINGFACEHUB_API_TOKEN ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # Embeddingsãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    if embedding_option == "HuggingFaceï¼ˆç„¡æ–™ï¼‰":
        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    else:
        embeddings = OpenAIEmbeddings(openai_api_key=api_key)

    # PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰UI
    uploaded_files = st.file_uploader("PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["pdf"], accept_multiple_files=True)

    if uploaded_files:
        all_text = ""
        total_pages = 0
        st.subheader("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸPDFãƒ•ã‚¡ã‚¤ãƒ«")

        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸPDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        for uploaded_file in uploaded_files:
            doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
            total_pages += doc.page_count
            st.markdown(f"- **{uploaded_file.name}**ï¼ˆ{doc.page_count}ãƒšãƒ¼ã‚¸ï¼‰")
            for page in doc:
                all_text += page.get_text() + "\n"

        st.info(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸPDFæ•°: {len(uploaded_files)}ã€åˆè¨ˆãƒšãƒ¼ã‚¸æ•°: {total_pages}")

        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ï¼ˆ1000æ–‡å­—ãƒ»100æ–‡å­—é‡è¤‡ï¼‰
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        docs = splitter.split_text(all_text)

        try:
            # Chromaãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–
            collection_name = f"pdf_demo_{uuid.uuid4()}"
            vectordb = Chroma.from_texts(docs, embeddings, collection_name=collection_name)
            retriever = vectordb.as_retriever(search_kwargs={"k": 3})

            # LLMã®é¸æŠã«å¿œã˜ã¦åˆæœŸåŒ–
            if llm_option == "OpenAI Chatãƒ¢ãƒ‡ãƒ«":
                llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, openai_api_key=api_key)
            elif llm_option == "Ollamaï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼‰":
                llm = OllamaLLM(model="mistral", temperature=0.7)
            else:
                llm = HFInferenceLLM()
            
            # æ—¥æœ¬èªå›ç­”ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
            japanese_prompt = PromptTemplate(
                input_variables=["context", "question"],
                template="""ä»¥ä¸‹ã®æ–‡è„ˆã‚’å‚è€ƒã«ã—ã¦è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
            æ–‡è„ˆ: {context}
            è³ªå•: {question}
            ç­”ãˆ:"""
            )

            # RAGè³ªå•å¿œç­”ãƒã‚§ãƒ¼ãƒ³ã®åˆæœŸåŒ–
            qa = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={"prompt": japanese_prompt}
            )

        except Exception as e:
            st.error("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯Chromaã®åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
            st.code(traceback.format_exc())
            st.stop()

        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
        if "messages" not in st.session_state:
            st.session_state.messages = []

        st.divider()
        st.subheader("ğŸ’¬ ãƒãƒ£ãƒƒãƒˆ")

        # éå»ã®ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(f'`{message["time"]}`: {message["content"]}')

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å—ã‘ä»˜ã‘
        user_input = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        if user_input:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
            st.session_state.messages.append({"role": "user", "content": user_input, "time": timestamp})
            with st.chat_message("user"):
                st.markdown(f'`{timestamp}`: {user_input}')

            # AIå›ç­”ã‚’å–å¾—ã—è¡¨ç¤º
            with st.chat_message("assistant"):
                with st.spinner("è€ƒãˆä¸­..."):
                    try:
                        answer = qa.invoke(user_input)
                        # è¾æ›¸ã§è¿”ã£ã¦ããŸã‚‰'result'ã ã‘è¦‹ã‚„ã™ãå–ã‚Šå‡ºã™
                        if isinstance(answer, dict) and "result" in answer:
                            answer_text = answer["result"]
                        else:
                            answer_text = str(answer)
                    except Exception as e:
                        answer_text = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼š\n\n```\n{str(e)}\n```"
                    st.markdown(answer_text)

            st.session_state.messages.append({"role": "assistant", "content": answer_text, "time": timestamp})

        st.divider()

        # RAGæ§‹æˆå›³ã®è¡¨ç¤ºï¼ˆMermaidå½¢å¼ï¼‰
        with st.expander("ğŸ§  RAGæ§‹æˆå›³ï¼ˆMermaidï¼‰"):
            embedding_label = "HuggingFace Embeddings" if "HuggingFace" in embedding_option else "OpenAI Embeddings"
            llm_label = {
                "OpenAI Chatãƒ¢ãƒ‡ãƒ«": "ChatGPT API",
                "Ollamaï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼‰": "Ollamaï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰",
                "HuggingFace Inferenceï¼ˆç„¡æ–™ï¼‰": "HuggingFace Inference"
            }[llm_option]

            mermaid_code = f'''
graph TD
    A[PDFãƒ•ã‚¡ã‚¤ãƒ«ç¾¤] --> B[ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆPyMuPDFï¼‰]
    B --> C[ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ï¼ˆLangChainï¼‰]
    C --> D[ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆ{embedding_label}ï¼‰]
    D --> E[Chroma VectorStore]
    E --> F[Retrieverï¼ˆä¼¼ãŸæ–‡ã‚’æ¤œç´¢ï¼‰]
    F --> G[{llm_label}]
    G --> H[Streamlitãƒãƒ£ãƒƒãƒˆUI]
'''
            st.code(mermaid_code, language="mermaid")

        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
        with st.expander("â¬‡ï¸ ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
            history_text = "\n\n".join(
                [f"[{m['time']}] {m['role'].capitalize()}: {m['content']}" for m in st.session_state.messages]
            )
            st.download_button("ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ä¿å­˜", history_text, file_name="chat_history.txt")


if __name__ == "__main__":
    main()